# -*- coding: utf-8 -*-
"""Sentiment_analysis .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17obaOGsIFTeruQrjkmPIBZoUcOG11iqD
"""

!pip install contractions

import pandas as pd
import re
import string
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
import contractions
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
#Chia tập train và test
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
#Huấn luyện Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
#Huấn luyện Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

df = pd.read_csv('/content/IMDB-Dataset.csv')
df = df.drop_duplicates()

stop = set(stopwords.words('english'))

#Expanding contractions
def expand_contractions(text):
    return contractions.fix(text)
def preprocess_text(text):
    wl = WordNetLemmatizer
    soup = BeautifulSoup(text,'html.parser')# Removing html tags
    text = soup.get_text()
    text = expand_contractions(text)# Expanding chatwords and contracts clearing contractions
    emoji_clean = re. compile ("["
                               u"\ U0001F600 -\ U0001F64F "
                               u"\ U0001F300 -\ U0001F5FF "
                               u"\ U0001F680 -\ U0001F6FF "
                               u"\ U0001F1E0 -\ U0001F1FF "
                               u"\ U00002702 -\ U000027B0 "
                               u"\ U000024C2 -\ U0001F251 "
                               "]+", flags =re. UNICODE )
    text = emoji_clean.sub(r'',text)
    text = re.sub (r'\.(?=\ S)', '. ',text ) #add space after full stop
    text = re.sub (r'http \S+', '', text ) # remove urls
    text = "". join ([
        word . lower () for word in text if word not in string . punctuation
    ]) # remove punctuation and make text lowercase
    text = " ".join([
    wl.lemmatizer.lemmatize(word) for word in text.split()
    if word not in stop and word.isalpha()])  # Chỉ giữ lại chữ cái
    return text

# Hàm tính phần trăm và giá trị tuyệt đối
def func(pct, allvalues):
    absolute = int(pct / 100. * np.sum(allvalues))
    return "{:.1f}%\n({:d})".format(pct, absolute)

# Lấy tần suất của các nhãn sentiment trong DataFrame
freq_pos = len(df[df['sentiment'] == 'positive'])
freq_neg = len(df[df['sentiment'] == 'negative'])

# Dữ liệu cho biểu đồ
data = [freq_pos, freq_neg]
labels = ['positive', 'negative']

# Tạo biểu đồ hình tròn
fig, ax = plt.subplots(figsize=[11, 7])
plt.pie(
    x=data,
    autopct=lambda pct: func(pct, data),  # Gọi hàm func để hiển thị phần trăm và số lượng
    explode=[0.0025] * 2,  # Làm cho các phần của biểu đồ "nổi lên" một chút
    pctdistance=0.5,  # Khoảng cách giữa nhãn và tâm biểu đồ
    colors=[sns.color_palette()[0], 'tab:red'],  # Màu sắc cho các phần của biểu đồ
    textprops={'fontsize': 16}  # Cài đặt kích thước chữ
)

# Đặt chú thích (legend)
labels = [r'Positive', r'Negative']
plt.legend(labels, loc="best", prop={'size': 14})

# Lưu biểu đồ dưới dạng file PNG
fig.savefig("PieChart.png")

# Hiển thị biểu đồ
plt.show()

# Tính chiều dài các từ trong cột 'review'
words_len = df['review'].str.split().map(lambda x: len(x))

# Sao chép DataFrame và thêm cột 'words length'
df_temp = df.copy()
df_temp['words_length'] = words_len

# Biểu đồ tần suất cho các đánh giá tiêu cực
hist_negative = sns.displot(
    data=df_temp[df_temp['sentiment'] == 'negative'],
    x="words_length", hue="sentiment", kde=True, height=7, aspect=1.1, legend=False, palette=['red']
).set(title='Words in negative reviews')

plt.show()
plt.figure(figsize=(7, 7.1))
kernel_distribution_number_words_plot = sns.kdeplot(
    data=df_temp, x="words_length", hue="sentiment", fill=True, palette=[sns.color_palette()[0], 'red']
).set(title='Words in reviews')

# Thêm chú thích (legend)
plt.legend(title='Sentiment', labels=['negative', 'positive'])

plt.show()

"""#Chia tập Train và Test"""

# Mã hóa nhãn 'sentiment' bằng LabelEncoder
label_encode = LabelEncoder()
y_data = label_encode.fit_transform(df['sentiment'])  # Sửa dấu ngoặc đơn

# Chuẩn bị dữ liệu đầu vào (x_data) sử dụng TfidfVectorizer
tfidf = TfidfVectorizer()
x_data = tfidf.fit_transform(df['review'])  # Vector hóa cột 'review'

# Chia dữ liệu thành tập huấn luyện và tập kiểm tra
x_train, x_test, y_train, y_test = train_test_split(
    x_data, y_data, test_size=0.2, random_state=42
)

"""#Biểu diễn văn bản thành vector"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Mã hóa nhãn sentiment
label_encode = LabelEncoder()
y_data = label_encode.fit_transform(df['sentiment'])

# Chia dữ liệu thành tập huấn luyện và kiểm tra
x_train, x_test, y_train, y_test = train_test_split(df['review'], y_data, test_size=0.2, random_state=42)

# Tạo đối tượng TfidfVectorizer với tối đa 10,000 đặc trưng
tfidf_vectorizer = TfidfVectorizer(max_features=10000)

# Huấn luyện vectorizer trên dữ liệu huấn luyện (chỉ sử dụng x_train)
tfidf_vectorizer.fit(x_train)

# Chuyển đổi dữ liệu huấn luyện và kiểm tra thành dạng vector TF-IDF
x_train_encoded = tfidf_vectorizer.transform(x_train)
x_test_encoded = tfidf_vectorizer.transform(x_test)


# Huấn luyện mô hình Decision Tree
dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt_classifier.fit(x_train_encoded, y_train)

# Dự đoán trên tập kiểm tra
y_pred_dt = dt_classifier.predict(x_test_encoded)

# Tính độ chính xác của mô hình
dt_accuracy = accuracy_score(y_test, y_pred_dt)
print(f"Decision Tree Accuracy: {dt_accuracy}")

"""#Huấn luyện Decision Tree Classifier"""

# Huấn luyện mô hình Decision Tree
dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt_classifier.fit(x_train_encoded, y_train)

# Dự đoán trên tập kiểm tra
y_pred_dt = dt_classifier.predict(x_test_encoded)

# Tính độ chính xác của mô hình
dt_accuracy = accuracy_score(y_test, y_pred_dt)
print(f"Decision Tree Accuracy: {dt_accuracy}")

"""#Huấn luyện Random Forest Classifier"""

rf_classifier = RandomForestClassifier(random_state=42)

# Huấn luyện mô hình với dữ liệu đã được mã hóa (TF-IDF)
rf_classifier.fit(x_train_encoded, y_train)

# Dự đoán trên tập kiểm tra
y_pred_rf = rf_classifier.predict(x_test_encoded)

# Tính độ chính xác của mô hình
rf_accuracy = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {rf_accuracy}")